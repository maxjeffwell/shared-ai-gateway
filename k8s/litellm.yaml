---
# LiteLLM Proxy - OpenAI-compatible proxy for multiple LLM providers
# Enables Lens Loop observability for Claude and Groq
apiVersion: v1
kind: ConfigMap
metadata:
  name: litellm-config
  labels:
    app: litellm
    component: ai-proxy
    tier: ai
data:
  config.yaml: |
    model_list:
      # Anthropic Claude models
      - model_name: claude-sonnet
        litellm_params:
          model: anthropic/claude-sonnet-4-20250514
          api_key: "os.environ/ANTHROPIC_API_KEY"
      - model_name: claude-sonnet-4-20250514
        litellm_params:
          model: anthropic/claude-sonnet-4-20250514
          api_key: "os.environ/ANTHROPIC_API_KEY"
      - model_name: claude-3-5-sonnet
        litellm_params:
          model: anthropic/claude-3-5-sonnet-20241022
          api_key: "os.environ/ANTHROPIC_API_KEY"
      # Groq models (for code-talk, educationelly apps)
      - model_name: groq-llama
        litellm_params:
          model: groq/llama-3.3-70b-versatile
          api_key: "os.environ/GROQ_API_KEY"
      - model_name: llama-3.3-70b-versatile
        litellm_params:
          model: groq/llama-3.3-70b-versatile
          api_key: "os.environ/GROQ_API_KEY"

    litellm_settings:
      drop_params: true
      set_verbose: false
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: litellm
  labels:
    app: litellm
    component: ai-proxy
    portfolio: "true"
    tier: ai
spec:
  replicas: 1
  selector:
    matchLabels:
      app: litellm
  template:
    metadata:
      labels:
        app: litellm
        component: ai-proxy
        portfolio: "true"
        tier: ai
    spec:
      containers:
        - name: litellm
          image: ghcr.io/berriai/litellm:main-latest
          args:
            - "--config"
            - "/app/config.yaml"
            - "--port"
            - "4000"
          ports:
            - containerPort: 4000
              name: http
              protocol: TCP
          env:
            - name: ANTHROPIC_API_KEY
              valueFrom:
                secretKeyRef:
                  name: anthropic-credentials
                  key: api-key
            - name: GROQ_API_KEY
              valueFrom:
                secretKeyRef:
                  name: groq-credentials
                  key: api-key
          resources:
            requests:
              cpu: 100m
              memory: 256Mi
            limits:
              cpu: 500m
              memory: 512Mi
          volumeMounts:
            - name: config
              mountPath: /app/config.yaml
              subPath: config.yaml
          livenessProbe:
            httpGet:
              path: /health/liveliness  # Don't use /health - it makes real API calls!
              port: 4000
            initialDelaySeconds: 30
            periodSeconds: 30
            timeoutSeconds: 10
          readinessProbe:
            httpGet:
              path: /health/readiness  # Don't use /health - it makes real API calls!
              port: 4000
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
      volumes:
        - name: config
          configMap:
            name: litellm-config
---
apiVersion: v1
kind: Service
metadata:
  name: litellm
  labels:
    app: litellm
    component: ai-proxy
    tier: ai
spec:
  type: ClusterIP
  selector:
    app: litellm
  ports:
    - name: http
      port: 4000
      targetPort: 4000
      protocol: TCP
---
# Network policy to allow cloudflared tunnel to reach LiteLLM
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-cloudflared-to-litellm
  labels:
    app: litellm
    component: ai-proxy
    tier: ai
spec:
  podSelector:
    matchLabels:
      app: litellm
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: cloudflared-ai-gateway
      ports:
        - protocol: TCP
          port: 4000
