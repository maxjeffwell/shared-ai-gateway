# Server Configuration
PORT=8002

# Backend preference: auto, local-gpu, runpod, local, anthropic
BACKEND_PREFERENCE=auto

# =============================================================================
# TIER 1: Local GPU (GTX 1080 via Ollama + Cloudflare tunnel)
# =============================================================================
# Your local machine running Ollama, exposed via Cloudflare tunnel
LOCAL_GPU_URL=https://gpu.yourdomain.com
LOCAL_GPU_MODEL=llama3.1:8b-instruct-q4_K_M

# =============================================================================
# TIER 2: RunPod GPU (RTX 4090 Serverless)
# =============================================================================
# Cloud GPU for when local is offline
RUNPOD_API_KEY=your-runpod-api-key
RUNPOD_ENDPOINT_ID=your-endpoint-id
RUNPOD_MODEL=meta-llama/Llama-3.1-8B-Instruct

# =============================================================================
# TIER 3: VPS CPU (Always Available Fallback)
# =============================================================================
# Llama.cpp server running on VPS
LOCAL_URL=http://llama-3b-service:8080

# =============================================================================
# ANTHROPIC (Claude - Premium Reasoning Tier)
# =============================================================================
# Use for complex tasks: K8s analysis, debugging, code review
# Request with backend: "anthropic" or "claude"
ANTHROPIC_API_KEY=your-anthropic-api-key
ANTHROPIC_MODEL=claude-sonnet-4-20250514

# =============================================================================
# EMBEDDING BACKENDS (2-Tier Fallback)
# =============================================================================
# Tier 1: Local GPU Triton (optional)
EMBEDDING_PRIMARY_URL=https://embeddings.yourdomain.com
# Tier 2: VPS CPU Triton (always available)
EMBEDDING_FALLBACK_URL=http://triton-embeddings:8000
EMBEDDING_MODEL=bge_embeddings

# =============================================================================
# REDIS CACHE
# =============================================================================
REDIS_URL=redis://redis:6379
CACHE_ENABLED=true
CACHE_TTL=3600

# =============================================================================
# GROQ (Free tier for education apps)
# =============================================================================
# Free API for code-talk, educationelly, educationelly-graphql
# Get API key at https://console.groq.com
GROQ_API_KEY=your-groq-api-key
GROQ_MODEL=llama-3.1-70b-versatile

# =============================================================================
# OBSERVABILITY (Lens Loop - Optional)
# =============================================================================
# LLM request tracing and analytics
LENS_LOOP_PROXY=http://host.docker.internal:31300
LENS_LOOP_PROJECT=lens-loop-project
